#!/usr/bin/env python3
"""
å€‹äººçŸ¥è­˜åº«æ•´ç†è…³æœ¬
- æ•´åˆç¢ç‰‡åŒ–çš„å°èªª/æ–‡æœ¬æ•¸æ“š
- éæ¿¾ä½è³ªé‡å…§å®¹(è¶…çŸ­ã€äº‚ç¢¼ã€ç²¾ç°¡ç‰ˆ)
- åˆ†é¡çµ„ç¹”é«˜è³ªé‡å…§å®¹
- ç”Ÿæˆåˆªé™¤å ±å‘Š
"""

import os
import re
import json
import shutil
from pathlib import Path
from collections import defaultdict
import hashlib

# é…ç½®
SOURCE_DIR = "/mnt/volume_sgp1_01/gcs_dump/vps-bomb/markdown"
OUTPUT_DIR = "/mnt/volume_sgp1_01/projects/personal-knowledge-base"
WASTE_DIR = os.path.join(OUTPUT_DIR, "å»¢æ–™")
BOOKS_DIR = os.path.join(OUTPUT_DIR, "books")

# è³ªé‡æ¨™æº–
MIN_FILE_SIZE = 10 * 1024  # 10KB æœ€å°æ–‡ä»¶å¤§å°
MIN_CONTENT_LENGTH = 5000  # æœ€å°‘5000å­—ç¬¦
MIN_CHAPTERS = 3  # æœ€å°‘ç« ç¯€æ•¸

# åƒåœ¾é—œéµè©
GARBAGE_KEYWORDS = [
    "test", "debug", "æ¸¬è©¦", "è°ƒè¯•",
    "hbmb", "yq", "test_"
]

class BookAnalyzer:
    def __init__(self):
        self.stats = {
            "total": 0,
            "processed": 0,
            "kept": 0,
            "removed": 0,
            "duplicates": 0
        }
        self.removed_books = []
        self.categories = defaultdict(list)
        self.seen_hashes = {}
        
    def is_garbage_filename(self, filename):
        """æª¢æŸ¥æ˜¯å¦ç‚ºåƒåœ¾æ–‡ä»¶å"""
        filename_lower = filename.lower()
        return any(kw in filename_lower for kw in GARBAGE_KEYWORDS)
    
    def detect_encoding_issues(self, content):
        """æª¢æ¸¬äº‚ç¢¼"""
        # æª¢æŸ¥æ˜¯å¦æœ‰å¤§é‡äº‚ç¢¼å­—ç¬¦
        garbage_chars = sum(1 for c in content if ord(c) > 0xFFFF or c == 'ï¿½')
        ratio = garbage_chars / len(content) if content else 1
        return ratio > 0.05  # è¶…é5%èªç‚ºæ˜¯äº‚ç¢¼
    
    def count_chapters(self, content):
        """çµ±è¨ˆç« ç¯€æ•¸"""
        chapter_patterns = [
            r'##\s+ç¬¬.{1,5}ç« ',
            r'ç¬¬.{1,5}ç« ',
            r'Chapter\s+\d+',
            r'##\s+\d+'
        ]
        count = 0
        for pattern in chapter_patterns:
            matches = re.findall(pattern, content)
            count = max(count, len(matches))
        return count
    
    def extract_title(self, filename, content):
        """æå–æ›¸å"""
        # å¾æ–‡ä»¶åæå–
        title = filename.replace('.md', '')
        
        # æ¸…ç†å¸¸è¦‹å¾Œç¶´
        title = re.sub(r'_TXTå°è¯´å¤©å ‚$', '', title)
        title = re.sub(r'_.*?\.txt$', '', title)
        title = re.sub(r'\.txt$', '', title)
        
        # å˜—è©¦å¾å…§å®¹æå–æ›´å¥½çš„æ¨™é¡Œ
        lines = content.split('\n')[:10]
        for line in lines:
            if line.startswith('# ') and len(line) > 2:
                extracted = line[2:].strip()
                if len(extracted) > len(title) * 0.5:
                    title = extracted
                break
        
        return title.strip()
    
    def categorize_book(self, title, content):
        """åˆ†é¡æ›¸ç±"""
        title_lower = title.lower()
        content_sample = content[:1000].lower()
        
        # ä½œè€…é—œéµè©
        authors = {
            "é˜¿åŠ è": "æ¨ç†æ‡¸ç–‘",
            "å²è’‚èŠ¬Â·é‡‘": "ææ€–é©šæ‚š",
            "å—æ´¾ä¸‰å”": "ç›œå¢“æ¢éšª",
            "å¤©ä¸‹éœ¸å”±": "ç›œå¢“æ¢éšª",
            "é¬¼é©¬æ˜Ÿ": "ææ€–é©šæ‚š",
        }
        
        for author, category in authors.items():
            if author in title:
                return category
        
        # é—œéµè©åˆ†é¡
        if any(kw in title_lower for kw in ['å¥³å°Š', 'ç©¿ä¹¦', 'é‡ç”Ÿ', 'ç©¿è¶Š']):
            return "ç¶²çµ¡å°èªª"
        elif any(kw in title_lower for kw in ['è°‹æ€', 'æ¢æ¡ˆ', 'ä¾¦æ¢', 'æ¨ç†']):
            return "æ¨ç†æ‡¸ç–‘"
        elif any(kw in title_lower for kw in ['ç›—å¢“', 'é¬¼å¹ç¯', 'å¤å¢“']):
            return "ç›œå¢“æ¢éšª"
        elif any(kw in title_lower for kw in ['ææ€–', 'æƒŠæ‚š', 'é¬¼', 'æ­»']):
            return "ææ€–é©šæ‚š"
        elif any(kw in title_lower for kw in ['å†›å¸ˆ', 'çš‡', 'å®«', 'æœ']):
            return "å¤ä»£è¨€æƒ…"
        else:
            return "å…¶ä»–"
    
    def get_content_hash(self, content):
        """è¨ˆç®—å…§å®¹å“ˆå¸Œç”¨æ–¼å»é‡"""
        # åªå–å‰10000å­—ç¬¦è¨ˆç®—å“ˆå¸Œ
        sample = content[:10000]
        return hashlib.md5(sample.encode('utf-8', errors='ignore')).hexdigest()
    
    def analyze_book(self, filepath):
        """åˆ†æå–®æœ¬æ›¸ç±"""
        self.stats["total"] += 1
        filename = os.path.basename(filepath)
        
        # è®€å–æ–‡ä»¶
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–: {filename} - {e}")
            self.removed_books.append({
                "filename": filename,
                "reason": f"è®€å–éŒ¯èª¤: {e}",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥1: åƒåœ¾æ–‡ä»¶å
        if self.is_garbage_filename(filename):
            print(f"ğŸ—‘ï¸  æ¸¬è©¦æ–‡ä»¶: {filename}")
            self.removed_books.append({
                "filename": filename,
                "reason": "æ¸¬è©¦/èª¿è©¦æ–‡ä»¶",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥2: æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)
        if file_size < MIN_FILE_SIZE:
            print(f"ğŸ—‘ï¸  æ–‡ä»¶éå°: {filename} ({file_size} bytes)")
            self.removed_books.append({
                "filename": filename,
                "reason": f"æ–‡ä»¶éå° ({file_size} bytes)",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥3: å…§å®¹é•·åº¦
        content_length = len(content)
        if content_length < MIN_CONTENT_LENGTH:
            print(f"ğŸ—‘ï¸  å…§å®¹éçŸ­: {filename} ({content_length} å­—ç¬¦)")
            self.removed_books.append({
                "filename": filename,
                "reason": f"å…§å®¹éçŸ­ ({content_length} å­—ç¬¦)",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥4: äº‚ç¢¼
        if self.detect_encoding_issues(content):
            print(f"ğŸ—‘ï¸  æª¢æ¸¬åˆ°äº‚ç¢¼: {filename}")
            self.removed_books.append({
                "filename": filename,
                "reason": "å…§å®¹äº‚ç¢¼",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥5: ç« ç¯€æ•¸
        chapter_count = self.count_chapters(content)
        if chapter_count < MIN_CHAPTERS and content_length < 50000:
            print(f"ğŸ—‘ï¸  ç« ç¯€éå°‘: {filename} ({chapter_count} ç« )")
            self.removed_books.append({
                "filename": filename,
                "reason": f"ç« ç¯€éå°‘ ({chapter_count} ç« )",
                "path": filepath
            })
            self.stats["removed"] += 1
            return None
        
        # æª¢æŸ¥6: å»é‡
        content_hash = self.get_content_hash(content)
        if content_hash in self.seen_hashes:
            print(f"ğŸ—‘ï¸  é‡è¤‡å…§å®¹: {filename}")
            self.removed_books.append({
                "filename": filename,
                "reason": f"èˆ‡ {self.seen_hashes[content_hash]} é‡è¤‡",
                "path": filepath
            })
            self.stats["duplicates"] += 1
            self.stats["removed"] += 1
            return None
        
        self.seen_hashes[content_hash] = filename
        
        # æå–ä¿¡æ¯
        title = self.extract_title(filename, content)
        category = self.categorize_book(title, content)
        
        print(f"âœ… ä¿ç•™: {title} [{category}] ({content_length} å­—ç¬¦, {chapter_count} ç« )")
        
        self.stats["kept"] += 1
        self.stats["processed"] += 1
        
        return {
            "filename": filename,
            "title": title,
            "category": category,
            "content": content,
            "length": content_length,
            "chapters": chapter_count,
            "source_path": filepath
        }
    
    def process_all_books(self):
        """è™•ç†æ‰€æœ‰æ›¸ç±"""
        print(f"\nğŸ“š é–‹å§‹æƒæ: {SOURCE_DIR}\n")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(BOOKS_DIR, exist_ok=True)
        os.makedirs(WASTE_DIR, exist_ok=True)
        
        # æƒææ‰€æœ‰markdownæ–‡ä»¶
        md_files = list(Path(SOURCE_DIR).rglob("*.md"))
        print(f"æ‰¾åˆ° {len(md_files)} å€‹æ–‡ä»¶\n")
        
        for filepath in md_files:
            book_info = self.analyze_book(str(filepath))
            
            if book_info:
                # ä¿å­˜åˆ°åˆ†é¡ç›®éŒ„
                category = book_info["category"]
                self.categories[category].append(book_info)
        
        print(f"\n{'='*60}")
        print(f"è™•ç†å®Œæˆ!")
        print(f"{'='*60}")
        print(f"ç¸½æ–‡ä»¶æ•¸: {self.stats['total']}")
        print(f"ä¿ç•™: {self.stats['kept']}")
        print(f"ç§»é™¤: {self.stats['removed']}")
        print(f"  - é‡è¤‡: {self.stats['duplicates']}")
        print(f"{'='*60}\n")
    
    def save_books(self):
        """ä¿å­˜æ•´ç†å¾Œçš„æ›¸ç±"""
        print("\nğŸ’¾ ä¿å­˜æ›¸ç±åˆ°åˆ†é¡ç›®éŒ„...\n")
        
        for category, books in self.categories.items():
            category_dir = os.path.join(BOOKS_DIR, category)
            os.makedirs(category_dir, exist_ok=True)
            
            print(f"ğŸ“ {category}: {len(books)} æœ¬")
            
            for book in books:
                # æ¸…ç†æ–‡ä»¶å
                safe_title = re.sub(r'[<>:"/\\|?*]', '_', book['title'])
                output_path = os.path.join(category_dir, f"{safe_title}.md")
                
                # å¯«å…¥æ–‡ä»¶
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(book['content'])
    
    def move_waste(self):
        """ç§»å‹•å»¢æ–™æ–‡ä»¶"""
        print(f"\nğŸ—‘ï¸  ç§»å‹• {len(self.removed_books)} å€‹å»¢æ–™æ–‡ä»¶...\n")
        
        for item in self.removed_books:
            source = item['path']
            if os.path.exists(source):
                # ä¿æŒåŸå§‹ç›®éŒ„çµæ§‹
                rel_path = os.path.relpath(source, SOURCE_DIR)
                dest = os.path.join(WASTE_DIR, rel_path)
                
                os.makedirs(os.path.dirname(dest), exist_ok=True)
                shutil.move(source, dest)
    
    def generate_reports(self):
        """ç”Ÿæˆå ±å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå ±å‘Š...\n")
        
        # 1. åˆªé™¤å ±å‘Š
        deletion_report = os.path.join(OUTPUT_DIR, "DELETION_REPORT.md")
        with open(deletion_report, 'w', encoding='utf-8') as f:
            f.write("# å»¢æ–™æ–‡ä»¶å ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {os.popen('date').read().strip()}\n\n")
            f.write(f"## çµ±è¨ˆ\n\n")
            f.write(f"- ç¸½æ–‡ä»¶æ•¸: {self.stats['total']}\n")
            f.write(f"- ä¿ç•™: {self.stats['kept']}\n")
            f.write(f"- ç§»é™¤: {self.stats['removed']}\n")
            f.write(f"- é‡è¤‡: {self.stats['duplicates']}\n\n")
            
            f.write("## è¢«ç§»é™¤çš„æ›¸ç±åˆ—è¡¨\n\n")
            f.write("| æ›¸å | åŸå›  |\n")
            f.write("|------|------|\n")
            
            for item in self.removed_books:
                f.write(f"| {item['filename']} | {item['reason']} |\n")
        
        # 2. åˆ†é¡ç›®éŒ„
        catalog = os.path.join(OUTPUT_DIR, "CATALOG.md")
        with open(catalog, 'w', encoding='utf-8') as f:
            f.write("# å€‹äººçŸ¥è­˜åº«ç›®éŒ„\n\n")
            f.write(f"ç¸½è¨ˆ: {self.stats['kept']} æœ¬æ›¸ç±\n\n")
            
            for category in sorted(self.categories.keys()):
                books = self.categories[category]
                f.write(f"## {category} ({len(books)})\n\n")
                
                for book in sorted(books, key=lambda x: x['title']):
                    f.write(f"- **{book['title']}** ")
                    f.write(f"({book['length']:,} å­—, {book['chapters']} ç« )\n")
                
                f.write("\n")
        
        # 3. JSONå…ƒæ•¸æ“š
        metadata = os.path.join(OUTPUT_DIR, "metadata.json")
        with open(metadata, 'w', encoding='utf-8') as f:
            data = {
                "stats": self.stats,
                "categories": {
                    cat: [{
                        "title": b['title'],
                        "filename": b['filename'],
                        "length": b['length'],
                        "chapters": b['chapters']
                    } for b in books]
                    for cat, books in self.categories.items()
                }
            }
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å ±å‘Šå·²ç”Ÿæˆ:")
        print(f"   - {deletion_report}")
        print(f"   - {catalog}")
        print(f"   - {metadata}")

def main():
    print("="*60)
    print("å€‹äººçŸ¥è­˜åº«æ•´ç†å·¥å…·")
    print("="*60)
    
    analyzer = BookAnalyzer()
    
    # è™•ç†æ‰€æœ‰æ›¸ç±
    analyzer.process_all_books()
    
    # ä¿å­˜æ•´ç†å¾Œçš„æ›¸ç±
    analyzer.save_books()
    
    # ç§»å‹•å»¢æ–™
    analyzer.move_waste()
    
    # ç”Ÿæˆå ±å‘Š
    analyzer.generate_reports()
    
    print("\nâœ¨ æ•´ç†å®Œæˆ!\n")

if __name__ == "__main__":
    main()
